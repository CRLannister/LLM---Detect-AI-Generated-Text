{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dependency and import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/nltk_data/corpora/wordnet.zip\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /tmp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /tmp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, statistics\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet', download_dir='/tmp/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/tmp/nltk_data')\n",
    "!unzip -n /tmp/nltk_data/corpora/wordnet.zip -d /tmp/nltk_data/corpora/\n",
    "\n",
    "# Specify the location of the NLTK data\n",
    "nltk.data.path.append('/tmp/nltk_data')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.metrics import distance\n",
    "\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and perform EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data//train_prompts.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# INPUT_DIR = '/kaggle/input/llm-detect-ai-generated-text'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m INPUT_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_prompts_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mINPUT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train_prompts.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_prompts_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data//train_prompts.csv'"
     ]
    }
   ],
   "source": [
    "# INPUT_DIR = '/kaggle/input/llm-detect-ai-generated-text'\n",
    "INPUT_DIR = 'data/'\n",
    "train_prompts_df = pd.read_csv(f'{INPUT_DIR}/train_prompts.csv')\n",
    "train_prompts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essays_df = pd.read_csv(f'{INPUT_DIR}/train_essays.csv')\n",
    "print(train_essays_df['prompt_id'].value_counts())\n",
    "print(train_essays_df['generated'].value_counts())\n",
    "train_essays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_essays_df = pd.read_csv(f'{INPUT_DIR}/test_essays.csv')\n",
    "test_essays_df.head()\n",
    "# train_essays_df.shape # (1378, 4)\n",
    "# test_essays_df.shape # (3, 3) Dummy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(f'{INPUT_DIR}/sample_submission.csv')\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essays_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PaLM_df = pd.read_csv(f'{INPUT_DIR}/../llm-generated-essay-using-palm-from-google-gen-ai/LLM_generated_essay_PaLM.csv')\n",
    "PaLM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PaLM_df['prompt_id'].value_counts())\n",
    "print(PaLM_df['generated'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([train_essays_df, PaLM_df], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "complete_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_df['prompt_id'].value_counts())\n",
    "print(complete_df['generated'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the wordnet tag from the part-of-speech tag\n",
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Define a function to check the spelling of a word\n",
    "def check_spelling(word):\n",
    "    # Check if the word is in the English dictionary\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Exclude punctuation and numeric values\n",
    "    if word in string.punctuation or word.isnumeric():\n",
    "        return True\n",
    "\n",
    "    # Check if the word is a stop word\n",
    "    if word.lower() in stop_words:\n",
    "        return True\n",
    "\n",
    "    if wordnet.synsets(word):\n",
    "        return True\n",
    "\n",
    "    # Otherwise, check the edit distance between the word and its closest match\n",
    "    else:\n",
    "        closest = wordnet.morphy(word)\n",
    "        if closest:\n",
    "            return distance.edit_distance(word, closest) <= 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def tokenize_by_punctuation(sentence):\n",
    "    # Use regular expression to tokenize by any punctuation\n",
    "    tokens = re.split(r'[,:;?!]', sentence)\n",
    "    # Remove empty strings from the result\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]\n",
    "    return tokens\n",
    "\n",
    "# Define a function to check the punctuation of a sentence\n",
    "def check_punctuation(sentence):\n",
    "    # Split the sentence by punctuation\n",
    "    segments = tokenize_by_punctuation(sentence)\n",
    "\n",
    "    # If there are more than one segment, check the punctuation rules\n",
    "    if len(segments) > 1:\n",
    "        # Loop through the segments\n",
    "        for i in range(len(segments) - 1):\n",
    "            # Get the punctuation mark between the segments\n",
    "            mark = sentence[sentence.index(segments[i]) + len(segments[i])]\n",
    "            \n",
    "            # Get the next segment\n",
    "            next_segment = segments[i + 1]\n",
    "\n",
    "            # If the punctuation mark is a comma, check if the next segment is a coordinating conjunction\n",
    "            if mark == \",\":\n",
    "                # Define a list of coordinating conjunctions\n",
    "                conjunctions = [\"and\", \"but\", \"or\", \"nor\", \"for\", \"yet\", \"so\"]\n",
    "                # If the next segment is not a conjunction, the comma is likely misplaced\n",
    "                if not nltk.word_tokenize(next_segment.strip().lower())[0] in conjunctions:\n",
    "                    return False\n",
    "                # If the next segment does not have a subject and a verb, the comma is likely misplaced\n",
    "                tags = nltk.pos_tag(nltk.word_tokenize(next_segment))\n",
    "                if not (\"NN\" in tags or \"NNS\" in tags or \"NNP\" in tags or \"NNPS\" in tags) and not (\"VB\" in tags or \"VBD\" in tags or \"VBG\" in tags or \"VBN\" in tags or \"VBP\" in tags or \"VBZ\" in tags):\n",
    "                    return False\n",
    "            # If the punctuation mark is a colon, check if the next segment is a complete sentence\n",
    "            elif mark == \":\":\n",
    "                # If the next segment does not have a subject and a verb, the colon is likely misplaced\n",
    "                tags = nltk.pos_tag(nltk.word_tokenize(next_segment))\n",
    "                if not (\"NN\" in tags or \"NNS\" in tags or \"NNP\" in tags or \"NNPS\" in tags) and not (\"VB\" in tags or \"VBD\" in tags or \"VBG\" in tags or \"VBN\" in tags or \"VBP\" in tags or \"VBZ\" in tags):\n",
    "                    return False\n",
    "            # If the punctuation mark is a semicolon, check if the next segment is a complete sentence\n",
    "            elif mark == \";\":\n",
    "                # If the next segment does not have a subject and a verb, the semicolon is likely misplaced\n",
    "                tags = nltk.pos_tag(nltk.word_tokenize(next_segment))\n",
    "                if not (\"NN\" in tags or \"NNS\" in tags or \"NNP\" in tags or \"NNPS\" in tags) and not (\"VB\" in tags or \"VBD\" in tags or \"VBG\" in tags or \"VBN\" in tags or \"VBP\" in tags or \"VBZ\" in tags):\n",
    "                    return False\n",
    "            # If the punctuation mark is a question mark, check if the sentence is a question\n",
    "            elif mark == \"?\":\n",
    "                # Define a list of question words\n",
    "                question_words = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"which\", \"whose\", \"whom\", \"can\", \"do\", \"does\"]\n",
    "                # If the sentence does not have a verb in the first position or a question word in any position, the question mark is likely misplaced\n",
    "                tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                if not (tags[0][1].startswith(\"V\") or any(tag[0].lower() in question_words for tag in tags)):\n",
    "                    return False\n",
    "            # If the punctuation mark is an exclamation mark, check if the sentence is an exclamation\n",
    "            elif mark == \"!\":\n",
    "                # Define a list of exclamatory words\n",
    "                exclamatory_words = [\"wow\", \"ouch\", \"yay\", \"oops\", \"ugh\", \"oh\", \"ah\", \"eh\", \"hmm\", \"no\", \"yes\"]\n",
    "                # If the sentence does not start with an exclamatory word, the exclamation mark is likely misplaced\n",
    "                if not nltk.word_tokenize(sentence.strip().lower())[0] in exclamatory_words:\n",
    "                    return False\n",
    "    # If there are no punctuation errors, return True\n",
    "    return True\n",
    "\n",
    "# Define a function to generate features about a text\n",
    "def generate_features(text):\n",
    "    # Initialize the feature dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Split the text into paragraphs\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by two newline characters\n",
    "\n",
    "    # Count the number of paragraphs\n",
    "    features['paragraph_count'] = len(paragraphs)\n",
    "    \n",
    "    total_word_count, total_sentence_count, total_spelling_errors, total_punctuation_errors, total_collocations = 0, 0 , 0, 0, 0\n",
    "\n",
    "    # Initialize lists to store values for calculating means\n",
    "    sentence_counts = []\n",
    "    word_counts = []\n",
    "    spelling_errors = []\n",
    "    punctuation_errors = []\n",
    "    collocations = []\n",
    "    most_common_word_frequencies = []\n",
    "    \n",
    "    # Apply word filter to remove stopwords and short words\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    filter_stops = lambda w: len(w) < 3 or w in stopset\n",
    "\n",
    "    # Loop through the paragraphs\n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        # Split the paragraph into sentences\n",
    "        sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "        # Count the number of sentences in the paragraph\n",
    "        sentence_counts.append(len(sentences))\n",
    "\n",
    "        # Loop through the sentences\n",
    "        for j, sentence in enumerate(sentences):\n",
    "            # Tokenize the sentence into words\n",
    "            words = nltk.word_tokenize(sentence)\n",
    "\n",
    "            # Count the number of words in the sentence\n",
    "            word_counts.append(len(words))\n",
    "\n",
    "            # Tag the words with their part-of-speech\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            # Lemmatize the words with their wordnet tag\n",
    "            lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "            lemmatized = [lemmatizer.lemmatize(word, get_wordnet_tag(tag)) for word, tag in tagged]\n",
    "\n",
    "            # Count the number of spelling errors in the sentence\n",
    "            spelling_errors.extend([not check_spelling(word) for word in words])\n",
    "\n",
    "            # Check the punctuation of the sentence\n",
    "            if not check_punctuation(sentence):\n",
    "                # If the sentence has punctuation errors, add it to the list\n",
    "                punctuation_errors.append(f'Punctuation error in paragraph {i+1}, sentence {j+1}: {sentence}')\n",
    "\n",
    "            # Create a text object from the lemmatized words\n",
    "            text_obj = nltk.Text(lemmatized)\n",
    "            \n",
    "            words_ = [w.lower() for w in sentence.split()]\n",
    "            \n",
    "            bigram_collocation = BigramCollocationFinder.from_words(words_)\n",
    "            bigram_collocation.apply_word_filter(filter_stops)\n",
    "\n",
    "            # Set a threshold for likelihood ratio \n",
    "            threshold = 10.0\n",
    "\n",
    "            # Get the bigrams with likelihood ratio above the threshold\n",
    "            significant_bigrams = [bigram for bigram, score in bigram_collocation.score_ngrams(BigramAssocMeasures.likelihood_ratio) if score > threshold]\n",
    "            if significant_bigrams:\n",
    "                collocations.extend(significant_bigrams)\n",
    "                            \n",
    "            # Find the collocations of the text\n",
    "            # if text_obj.collocations():\n",
    "            #    collocations.extend(text_obj.collocations())\n",
    "\n",
    "            # Count the number of spelling errors in the sentence\n",
    "            features[f'spelling_errors_paragraph_{i+1}_sentence_{j+1}'] = sum([not check_spelling(word) for word in words])\n",
    "\n",
    "            # Count the number of punctuation errors in the sentence\n",
    "            features[f'punctuation_errors_paragraph_{i+1}_sentence_{j+1}'] = not check_punctuation(sentence)\n",
    "\n",
    "        # Calculate mean values for the current paragraph\n",
    "        len_sentence_count = len(sentence_counts)\n",
    "        len_word_counts = len(word_counts)\n",
    "        len_spelling_errors = len(spelling_errors)\n",
    "        len_punctuation_errors = len(punctuation_errors)\n",
    "        \n",
    "        if len_sentence_count == 0: len_sentence_count = 1\n",
    "        if len_word_counts == 0: len_word_counts = 1\n",
    "        if len_spelling_errors == 0: len_spelling_errors = 1\n",
    "        if len_punctuation_errors == 0: len_punctuation_errors = 1\n",
    "        \n",
    "        \n",
    "        features[f'mean_sentence_count_paragraph_{i+1}'] = sum(sentence_counts) / len_sentence_count\n",
    "        features[f'mean_word_count_paragraph_{i+1}'] = sum(word_counts) / len_word_counts\n",
    "        features[f'mean_spelling_errors_paragraph_{i+1}'] = sum(spelling_errors) / len_spelling_errors\n",
    "        features[f'mean_punctuation_errors_paragraph_{i+1}'] = punctuation_errors.count(True) / len_punctuation_errors\n",
    "        features[f'mean_collocations_paragraph_{i+1}'] = len(collocations) / len_sentence_count\n",
    "\n",
    "        # Calculate total values for the current paragraph\n",
    "        features[f'total_spelling_errors_paragraph_{i+1}'] = sum(spelling_errors)\n",
    "        features[f'total_punctuation_errors_paragraph_{i+1}'] = punctuation_errors.count(True)\n",
    "        features[f'total_collocations_paragraph_{i+1}'] = len(collocations)\n",
    "        \n",
    "        total_word_count +=  sum(word_counts)\n",
    "        total_sentence_count += sum(sentence_counts)\n",
    "        total_spelling_errors += sum(spelling_errors)\n",
    "        total_punctuation_errors += punctuation_errors.count(True)\n",
    "        total_collocations += len(collocations)\n",
    "        \n",
    "        # Reset lists for the next paragraph\n",
    "        sentence_counts = []\n",
    "        word_counts = []\n",
    "        spelling_errors = []\n",
    "        punctuation_errors = []\n",
    "        collocations = []\n",
    "\n",
    "    total_paragraphs = len(paragraphs)\n",
    "    features_dict = {\n",
    "        'paragraph_count': total_paragraphs,\n",
    "        'word_count': total_word_count,\n",
    "        'sentence_count': total_sentence_count,\n",
    "        'spelling_errors': total_spelling_errors,\n",
    "        'punctuation_errors': total_punctuation_errors,\n",
    "        'collocations': total_collocations\n",
    "    }\n",
    "        \n",
    "    # Calculate overall mean values\n",
    "    features_dict['mean_sentence_count'] = sum(features.get(f'mean_sentence_count_paragraph_{i+1}', 0) for i in range(total_paragraphs)) / total_paragraphs if total_paragraphs > 0 else 0\n",
    "    features_dict['mean_word_count'] = sum(features.get(f'mean_word_count_paragraph_{i+1}', 0) for i in range(total_paragraphs)) / total_paragraphs if total_paragraphs > 0 else 0\n",
    "    features_dict['mean_spelling_errors'] = sum(features.get(f'mean_spelling_errors_paragraph_{i+1}', 0) for i in range(total_paragraphs)) / total_paragraphs if total_paragraphs > 0 else 0\n",
    "    features_dict['mean_punctuation_errors'] = sum(features.get(f'mean_punctuation_errors_paragraph_{i+1}', 0) for i in range(total_paragraphs)) / total_paragraphs if total_paragraphs > 0 else 0\n",
    "    features_dict['mean_collocations'] = sum(features.get(f'mean_collocations_paragraph_{i+1}', 0) for i in range(total_paragraphs)) / total_paragraphs if total_paragraphs > 0 else 0\n",
    "\n",
    "    return features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[\"features\"] = complete_df['text'].apply(generate_features)\n",
    "complete_df = pd.concat([complete_df.drop(['features'], axis=1), complete_df['features'].apply(pd.Series)], axis=1)\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_features = ['paragraph_count', 'word_count', 'sentence_count', 'spelling_errors', \n",
    "                        'punctuation_errors', 'collocations', 'mean_sentence_count', \n",
    "                        'mean_word_count', 'mean_spelling_errors', 'mean_punctuation_errors', \n",
    "                        'mean_collocations']\n",
    "\n",
    "Complete_required_df = complete_df[additional_features + ['id', 'text', 'generated']]\n",
    "X_train_required_df, X_test_required_df, y_train_required_df, y_test_required_df = train_test_split(\n",
    "    Complete_required_df[additional_features + ['text']], Complete_required_df['generated'],\n",
    "    test_size=0.2,  \n",
    "    stratify=complete_df['generated'],\n",
    "    random_state=42  \n",
    ")\n",
    "\n",
    "print(X_train_required_df.shape, y_train_required_df.shape)\n",
    "print(X_test_required_df.shape, y_test_required_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_required_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_required_df.value_counts())\n",
    "print(y_test_required_df.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distibution and Plots between Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each essay and create a new column\n",
    "complete_df['essay_length'] = complete_df['text'].apply(len)\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Distribution of essay lengths for student essays\n",
    "sns.histplot(complete_df[complete_df['generated'] == 0]['essay_length'], color=\"skyblue\", label='Student Essays', kde=True)\n",
    "\n",
    "# Distribution of essay lengths for LLM generated essays\n",
    "sns.histplot(complete_df[complete_df['generated'] == 1]['essay_length'], color=\"red\", label='LLM Generated Essays', kde=True)\n",
    "\n",
    "plt.title('Distribution of Essay Lengths')\n",
    "plt.xlabel('Essay Length (Number of Characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Distribution of spelling_errors for student essays\n",
    "sns.histplot(complete_df[complete_df['generated'] == 0]['spelling_errors'], color=\"skyblue\", label='Student Essays', kde=True)\n",
    "\n",
    "# Distribution of spelling_errors for LLM generated essays\n",
    "sns.histplot(complete_df[complete_df['generated'] == 1]['spelling_errors'], color=\"red\", label='LLM Generated Essays', kde=True)\n",
    "\n",
    "plt.title('Distribution of spelling_errors')\n",
    "plt.xlabel('spelling_errors')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between engineered features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "# Define a colormap\n",
    "royalblue = LinearSegmentedColormap.from_list('royalblue', [(0, (1,1,1)), (1, (0.25,0.41,0.88))])\n",
    "royalblue_r = royalblue.reversed()\n",
    "\n",
    "target = 'generated'\n",
    "numeric_columns = additional_features + ['generated']\n",
    "complete_df_numeric = complete_df[numeric_columns]\n",
    "\n",
    "complete_df_numeric_ordered = pd.concat([complete_df_numeric.drop(target,axis=1), complete_df_numeric[target]],axis=1)\n",
    "\n",
    "corr = complete_df_numeric_ordered.corr(method='spearman')\n",
    "\n",
    "# Create a mask so that we see the correlation values only once\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask,1)] = True\n",
    "\n",
    "# Plot the heatmap correlation\n",
    "plt.figure(figsize=(12,8), dpi=80)\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap=royalblue, fmt='.2f', linewidths=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(['royalblue', 'darkturquoise'])\n",
    "\n",
    "Num_Features = additional_features + ['generated']\n",
    "\n",
    "fig, ax = plt.subplots(len(Num_Features), 2, figsize=(30,75), dpi=200, gridspec_kw={'width_ratios': [1, 2]})\n",
    "target = 'generated'\n",
    "\n",
    "\n",
    "for i,col in enumerate(Num_Features):\n",
    "    # barplot\n",
    "    graph = sns.barplot(data=complete_df_numeric, x=target, y=col, ax=ax[i,0])\n",
    "    # kde Plot\n",
    "    sns.kdeplot(data=complete_df_numeric[complete_df_numeric[target]<=0.5], x=col, fill=True, linewidth=2, ax=ax[i,1], label='0.0')\n",
    "    sns.kdeplot(data=complete_df_numeric[complete_df_numeric[target]>=0.5], x=col, fill=True, linewidth=2, ax=ax[i,1], label='1.0')\n",
    "    ax[i,1].set_yticks([])\n",
    "    ax[i,1].legend(title='generated', loc='upper right')\n",
    "    # Add bar sizes to our plot\n",
    "    for cont in graph.containers:\n",
    "        graph.bar_label(cont, fmt='         %.3g')\n",
    "\n",
    "plt.suptitle('Numerical Features vs Target Distribution', fontsize=22)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['collocations'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing TF-IDF to populate training Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)\n",
    "X_train_text = vectorizer.fit_transform(X_train_required_df['text'])\n",
    "\n",
    "X_train_required_df.reset_index(drop= True, inplace=True)\n",
    "\n",
    "# Combine text data with additional features\n",
    "X_train = pd.concat([pd.DataFrame(X_train_text.toarray()), X_train_required_df[additional_features]], axis=1)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Training features and Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "auc_scores = []\n",
    "\n",
    "X_train.columns = X_train.columns.astype(str)\n",
    "column_order = X_train.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=column_order)\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the training data with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_idx, val_idx in cv.split(X_train, y_train_required_df):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train_required_df.iloc[train_idx], y_train_required_df.iloc[val_idx]\n",
    "\n",
    "    lr_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    preds_val_lr = lr_model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    auc_score = roc_auc_score(y_val_fold, preds_val_lr)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "for i, score in enumerate(auc_scores, 1):\n",
    "    print(f'ROC AUC for fold {i}: {score:.4f}')\n",
    "\n",
    "print('Average ROC AUC:', round(sum(auc_scores)/len(auc_scores), 4))\n",
    "print('Standard deviation:', round((sum([(x - sum(auc_scores)/len(auc_scores))**2 for x in auc_scores])/len(auc_scores))**0.5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_text = vectorizer.transform(X_test_required_df['text'])\n",
    "\n",
    "X_test_required_df.reset_index(drop= True, inplace=True)\n",
    "\n",
    "X_test = pd.concat([pd.DataFrame(X_test_text.toarray()), X_test_required_df[additional_features]], axis=1)\n",
    "\n",
    "X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the testing Data from the fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = lr_model.predict(X_test_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the positive class on the test data\n",
    "test_probs = lr_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC score for the test set\n",
    "roc_auc = roc_auc_score(y_test_required_df, test_probs)\n",
    "\n",
    "print('ROC AUC on Test Set:', round(roc_auc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Metrics for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test_required_df\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, test_preds)\n",
    "\n",
    "# Extracting TP, FP, FN, TN\n",
    "TP = conf_matrix[1, 1]\n",
    "FP = conf_matrix[0, 1]\n",
    "FN = conf_matrix[1, 0]\n",
    "TN = conf_matrix[0, 0]\n",
    "\n",
    "# Other metrics\n",
    "accuracy = accuracy_score(y_true, test_preds)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, test_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Test Data\n",
    "test_essays_df[\"features\"] = test_essays_df['text'].apply(generate_features)\n",
    "test_essays_df = pd.concat([test_essays_df.drop(['features'], axis=1), test_essays_df['features'].apply(pd.Series)], axis=1)\n",
    "test_essays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_essays_df[['id']]\n",
    "test_essays_df = test_essays_df[additional_features + ['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to change test data to feed into model\n",
    "X_submission_text = vectorizer.transform(test_essays_df['text'])\n",
    "\n",
    "test_essays_df.reset_index(drop= True, inplace=True)\n",
    "\n",
    "X_submission = pd.concat([pd.DataFrame(X_submission_text.toarray()), test_essays_df[additional_features]], axis=1)\n",
    "\n",
    "X_submission.columns = X_submission.columns.astype(str)\n",
    "\n",
    "X_submission_scaled = scaler.transform(X_submission)\n",
    "\n",
    "X_submission_scaled_df = pd.DataFrame(X_submission_scaled, columns=column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = lr_model.predict_proba(X_submission_scaled_df)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['generated'] = test_probs\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3973977,
     "sourceId": 6920799,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
